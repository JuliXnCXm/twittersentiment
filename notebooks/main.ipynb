{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in /home/blindedsoul/.local/lib/python3.8/site-packages (0.4.3.20220106)\n",
      "Requirement already satisfied: filelock in /home/blindedsoul/.local/lib/python3.8/site-packages (from snscrape) (3.6.0)\n",
      "Requirement already satisfied: pytz; python_version < \"3.9.0\" in /usr/lib/python3/dist-packages (from snscrape) (2019.3)\n",
      "Requirement already satisfied: lxml in /home/blindedsoul/.local/lib/python3.8/site-packages (from snscrape) (4.6.3)\n",
      "Requirement already satisfied: requests[socks] in /home/blindedsoul/.local/lib/python3.8/site-packages (from snscrape) (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests[socks]->snscrape) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->snscrape) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests[socks]->snscrape) (2.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->snscrape) (2.2.1)\n",
      "zsh:1: = not found\n",
      "Collecting NRCLex\n",
      "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
      "\u001b[K     |████████████████████████████████| 396 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: textblob in /home/blindedsoul/.local/lib/python3.8/site-packages (from NRCLex) (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from textblob->NRCLex) (3.6.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1; python_version >= \"3\"->textblob->NRCLex) (4.62.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1; python_version >= \"3\"->textblob->NRCLex) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1; python_version >= \"3\"->textblob->NRCLex) (2021.11.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1; python_version >= \"3\"->textblob->NRCLex) (7.0)\n",
      "Building wheels for collected packages: NRCLex\n",
      "  Building wheel for NRCLex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43310 sha256=9d4978f615c46e7e01faa1448f6e7b9b55b208fa12ac0a01321dfd3c0a9c4be1\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/83/95/c0/42b43fb15eb48e4f5a67cba8915540cb2783591c59c037a9e5\n",
      "Successfully built NRCLex\n",
      "Installing collected packages: NRCLex\n",
      "Successfully installed NRCLex-3.0.0\n",
      "zsh:1: = not found\n",
      "Collecting text2emotion\n",
      "  Using cached text2emotion-0.0.5-py3-none-any.whl (57 kB)\n",
      "Collecting emoji>=0.6.0\n",
      "  Using cached emoji-1.7.0.tar.gz (175 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from text2emotion) (3.6.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->text2emotion) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->text2emotion) (2021.11.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->text2emotion) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->text2emotion) (1.1.0)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171029 sha256=3e39e02ba0e05ff7e1ff22882fef21c6d3b5f1fc692b97dca00343ea0846d555\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/5e/8c/80/c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, text2emotion\n",
      "Successfully installed emoji-1.7.0 text2emotion-0.0.5\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/blindedsoul/.local/lib/python3.8/site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests->vaderSentiment) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->vaderSentiment) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests->vaderSentiment) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests->vaderSentiment) (2.8)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Collecting twittersentiment\n",
      "  Downloading twittersentiment-0.0.3-py3-none-any.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 3.7 MB/s eta 0:00:01     |██████████████▋                 | 1.6 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow==2.2.0\n",
      "  Downloading tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl (516.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.3 MB 597 bytes/s a 0:00:01   |█                               | 16.7 MB 1.8 MB/s eta 0:04:31     |█▋                              | 26.6 MB 1.8 MB/s eta 0:04:26     |███▉                            | 61.8 MB 19.5 MB/s eta 0:00:24     |███████                         | 112.7 MB 28.2 MB/s eta 0:00:15     |████████▊                       | 140.8 MB 15.3 MB/s eta 0:00:25     |██████████▋                     | 170.8 MB 28.9 MB/s eta 0:00:12     |████████████▋                   | 203.6 MB 20.4 MB/s eta 0:00:16     |██████████████████▎             | 294.2 MB 280 kB/s eta 0:13:13     |███████████████████             | 307.0 MB 23.1 MB/s eta 0:00:10     |████████████████████            | 322.7 MB 23.1 MB/s eta 0:00:09     |█████████████████████████▌      | 411.2 MB 17.2 MB/s eta 0:00:07 | 433.5 MB 534 kB/s eta 0:02:35     |███████████████████████████████▊| 512.7 MB 43.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.19.0\n",
      "  Downloading numpy-1.19.0-cp38-cp38-manylinux2010_x86_64.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 4.6 MB/s eta 0:00:01     |█████████████████████████       | 11.4 MB 4.6 MB/s eta 0:00:01     |███████████████████████████████ | 14.2 MB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.5.0\n",
      "  Downloading torch-1.5.0-cp38-cp38-manylinux1_x86_64.whl (752.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 752.0 MB 816 bytes/s a 0:00:014   |███▍                            | 79.6 MB 27.5 MB/s eta 0:00:25     |████▏                           | 97.0 MB 15.9 MB/s eta 0:00:42     |█████                           | 116.2 MB 13.8 MB/s eta 0:00:47     |█████▊                          | 134.1 MB 12.3 MB/s eta 0:00:51     |██████                          | 141.8 MB 12.3 MB/s eta 0:00:50MB 12.3 MB/s eta 0:00:50     |█████████▍                      | 221.4 MB 22.8 MB/s eta 0:00:24     |██████████▋                     | 248.8 MB 21.5 MB/s eta 0:00:24      | 280.2 MB 344 kB/s eta 0:22:51     |█████████████▎                  | 311.4 MB 18.9 MB/s eta 0:00:24     |████████████████                | 377.8 MB 15.3 MB/s eta 0:00:25     |█████████████████▌              | 411.2 MB 25.0 MB/s eta 0:00:14��████████              | 420.6 MB 571 kB/s eta 0:09:41[K     |██████████████████▋             | 437.8 MB 571 kB/s eta 0:09:11     |███████████████████▊            | 463.7 MB 90.2 MB/s eta 0:00:04     |███████████████████▉            | 465.2 MB 90.2 MB/s eta 0:00:04     |████████████████████            | 471.0 MB 17.5 MB/s eta 0:00:17     |████████████████████████        | 562.5 MB 21.5 MB/s eta 0:00:09     |██████████████████████████      | 610.5 MB 25.0 MB/s eta 0:00:06     |██████████████████████████▌     | 623.7 MB 25.0 MB/s eta 0:00:06     |███████████████████████████▎    | 641.3 MB 27.7 MB/s eta 0:00:05     |███████████████████████████▉    | 654.4 MB 27.7 MB/s eta 0:00:04     |████████████████████████████▏   | 662.7 MB 896 kB/s eta 0:01:40     |████████████████████████████▌   | 670.1 MB 896 kB/s eta 0:01:32     |█████████████████████████████   | 681.8 MB 896 kB/s eta 0:01:19\n",
      "\u001b[?25hCollecting pandas==1.1.0\n",
      "  Downloading pandas-1.1.0-cp38-cp38-manylinux1_x86_64.whl (10.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.3 MB 19 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting torchtext==0.5.0\n",
      "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 83 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk==3.5\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 44.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==0.23.0\n",
      "  Downloading scikit_learn-0.23.0-cp38-cp38-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 29.8 MB/s eta 0:00:01  | 1.5 MB 29.8 MB/s eta 0:00:01/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0 MB 60 kB/s  eta 0:00:01109     |█████████                       | 7.4 MB 2.8 MB/s eta 0:00:07     |█████████████                   | 10.6 MB 2.8 MB/s eta 0:00:06     |█████████████████████▏          | 17.2 MB 17.0 MB/s eta 0:00:01     |███████████████████████▉        | 19.3 MB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 83.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/lib/python3/dist-packages (from tensorflow==2.2.0->twittersentiment) (0.34.2)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.14.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/blindedsoul/.local/lib/python3.8/site-packages (from tensorflow==2.2.0->twittersentiment) (1.16.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from torch==1.5.0->twittersentiment) (0.18.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas==1.1.0->twittersentiment) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas==1.1.0->twittersentiment) (2.7.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.5.0->twittersentiment) (4.62.3)\n",
      "Requirement already satisfied: requests in /home/blindedsoul/.local/lib/python3.8/site-packages (from torchtext==0.5.0->twittersentiment) (2.27.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk==3.5->twittersentiment) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk==3.5->twittersentiment) (1.1.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from nltk==3.5->twittersentiment) (2021.11.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->twittersentiment) (45.2.0)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext==0.5.0->twittersentiment) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests->torchtext==0.5.0->twittersentiment) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests->torchtext==0.5.0->twittersentiment) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests->torchtext==0.5.0->twittersentiment) (1.26.5)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->twittersentiment) (0.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/blindedsoul/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->twittersentiment) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->twittersentiment) (1.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->twittersentiment) (0.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->twittersentiment) (3.2.0)\n",
      "Building wheels for collected packages: nltk, termcolor\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=df98a7f52fbecc58d62064026ce91528ac0f65421f3143c72314dab42e9960ab\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=a30c15f67258e93499b7f225e67942c6899a823101cb17f9b31e724236abf527\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built nltk termcolor\n",
      "Installing collected packages: numpy, gast, scipy, tensorflow-estimator, google-pasta, wrapt, keras-preprocessing, opt-einsum, grpcio, h5py, protobuf, absl-py, tensorboard-plugin-wit, importlib-metadata, markdown, cachetools, rsa, google-auth, werkzeug, google-auth-oauthlib, tensorboard, astunparse, termcolor, tensorflow, torch, pandas, sentencepiece, torchtext, nltk, threadpoolctl, scikit-learn, twittersentiment\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.2\n",
      "    Uninstalling pandas-1.3.2:\n",
      "      Successfully uninstalled pandas-1.3.2\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 gast-0.3.3 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-2.10.0 importlib-metadata-4.11.3 keras-preprocessing-1.1.2 markdown-3.3.6 nltk-3.5 numpy-1.19.0 opt-einsum-3.3.0 pandas-1.1.0 protobuf-3.19.4 rsa-4.8 scikit-learn-0.23.0 scipy-1.4.1 sentencepiece-0.1.96 tensorboard-2.2.2 tensorboard-plugin-wit-1.8.1 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0 threadpoolctl-3.1.0 torch-1.5.0 torchtext-0.5.0 twittersentiment-0.0.3 werkzeug-2.0.3 wrapt-1.14.0\n",
      "Collecting flair\n",
      "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
      "\u001b[K     |████████████████████████████████| 322 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/lib/python3/dist-packages (from flair) (2.7.3)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7 MB 8.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /home/blindedsoul/.local/lib/python3.8/site-packages (from flair) (0.23.0)\n",
      "Requirement already satisfied: lxml in /home/blindedsoul/.local/lib/python3.8/site-packages (from flair) (4.6.3)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in /home/blindedsoul/.local/lib/python3.8/site-packages (from flair) (1.5.0)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting more-itertools~=8.8.0\n",
      "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 3.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting gdown==3.12.2\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.62.3)\n",
      "Collecting wikipedia-api\n",
      "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "\u001b[K     |████████████████████████████████| 788 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 1.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from flair) (2021.11.2)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
      "Collecting transformers>=4.0.0\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gensim>=3.4.0\n",
      "  Downloading gensim-4.1.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1 MB 69 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.8/dist-packages (from flair) (3.4.3)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 27.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/blindedsoul/.local/lib/python3.8/site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/blindedsoul/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/blindedsoul/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/blindedsoul/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair) (1.19.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from torch!=1.8,>=1.5.0->flair) (0.18.2)\n",
      "Requirement already satisfied: filelock in /home/blindedsoul/.local/lib/python3.8/site-packages (from gdown==3.12.2->flair) (3.6.0)\n",
      "Requirement already satisfied: six in /home/blindedsoul/.local/lib/python3.8/site-packages (from gdown==3.12.2->flair) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in /home/blindedsoul/.local/lib/python3.8/site-packages (from gdown==3.12.2->flair) (2.27.1)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/blindedsoul/.local/lib/python3.8/site-packages (from deprecated>=1.2.4->flair) (1.14.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/blindedsoul/.local/lib/python3.8/site-packages (from huggingface-hub->flair) (21.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from huggingface-hub->flair) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->flair) (3.10.0.2)\n",
      "Collecting tokenizers!=0.11.3,>=0.11.1\n",
      "  Downloading tokenizers-0.11.6-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 3.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib>=2.2.3->flair) (7.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/blindedsoul/.local/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests[socks]->gdown==3.12.2->flair) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests[socks]->gdown==3.12.2->flair) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /home/blindedsoul/.local/lib/python3.8/site-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (1.0.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.0)\n",
      "Building wheels for collected packages: sqlitedict, gdown, wikipedia-api, mpld3, langdetect, overrides\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15717 sha256=fdc9029e9735aa0a74a0ed812936cbc88b7c9aa65bc097b19cd890c8cb9b30bb\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/ee/0b/8c/3cdf3e7eef4161d79c62df5bef35b0614238d0d2bd3051877a\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9681 sha256=0a29ccb3af1963104246eb9814126a4de53f7f52245156d13605c16c5b28e79b\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/e2/62/1e/926d1ebe7b1e733c78d627fd288d01b83feaf67efc06e0e4c3\n",
      "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13462 sha256=5ea796f6ce617cfdfe25bb28755915ae5a1bb545876cc7499d430163dbb1e01c\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/ed/88/e3/da3d4d73cb91d659488cfa25913b84bbc26febec99d257bce9\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116678 sha256=643dc5f6270abe056fca215208d01930407e5101f16774c33e4b894f05af7be8\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/3d/9f/9d/d806a20bd97bc7076d724fa3e69fa5be61836ba16b2ffa6126\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=618ec03fb4cf1eba24a603d53b45fe07b12be2222721f84469dabf36729a291a\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10173 sha256=d1b32ff25ab999ed14455829238f4f88ba4a1cc2413c290fdc5208dcc5a86f25\n",
      "  Stored in directory: /home/blindedsoul/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
      "Successfully built sqlitedict gdown wikipedia-api mpld3 langdetect overrides\n",
      "\u001b[31mERROR: markdown 3.3.6 has requirement importlib-metadata>=4.4; python_version < \"3.10\", but you'll have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: sentencepiece, sqlitedict, ftfy, janome, segtok, more-itertools, gdown, overrides, importlib-metadata, konoha, conllu, wikipedia-api, mpld3, deprecated, huggingface-hub, smart-open, gensim, bpemb, tokenizers, sacremoses, transformers, langdetect, tabulate, flair\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.96\n",
      "    Uninstalling sentencepiece-0.1.96:\n",
      "      Successfully uninstalled sentencepiece-0.1.96\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.10 ftfy-6.1.1 gdown-3.12.2 gensim-4.1.2 huggingface-hub-0.4.0 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.49 segtok-1.5.11 sentencepiece-0.1.95 smart-open-5.2.1 sqlitedict-2.0.0 tabulate-0.8.9 tokenizers-0.11.6 transformers-4.17.0 wikipedia-api-0.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape\n",
    "!pip install googletrans == 3.1.0a0\n",
    "!pip install NRCLex\n",
    "!pip install tweepy == 4.6.0\n",
    "!pip install text2emotion\n",
    "!pip install vaderSentiment\n",
    "!pip install twittersentiment\n",
    "!pip install flair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/blindedsoul/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/blindedsoul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import snscrape.modules.twitter as sninstagram\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from nrclex import NRCLex\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import text2emotion as te\n",
    "from googletrans import Translator\n",
    "import matplotlib.pyplot as plt\n",
    "from twittersentiment import TwitterSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(\"text-query-tweets.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Using OS library to call CLI commands in Python\n",
    "os.system(\"snscrape --jsonl --max-results 500 --since 2021-01-21 twitter-search 'paro contra rappi until:2022-03-28' > text-query-tweets.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXTBLOB PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets...\n",
      "Reading file...\n",
      "Reading file...\n"
     ]
    }
   ],
   "source": [
    "class TwitterScrapeSentimentAnalysis():\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.sinceDate = \"2022-01-01\"\n",
    "            self.untilDate = \"2022-03-13\"\n",
    "            self.hashtag = \"\"\n",
    "        except:\n",
    "            print(\"Error: scraper Failed\")\n",
    "\n",
    "    def read_file(self,hashtag, since, until):\n",
    "        print(\"Reading file...\")\n",
    "        os.system(\"snscrape --jsonl --max-results 500 --since {since} twitter-hashtag '{hashtag} until:{until}' > text-query-tweets.json\".format(hashtag=hashtag , since = since , until = until))\n",
    "        print(\"Reading file...\")\n",
    "        tweets = pd.read_json(\"text-query-tweets.json\", lines=True)\n",
    "        return tweets\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
    "\n",
    "    def get_sentiment_tweet(self, tweet):\n",
    "        translator = Translator()\n",
    "        tweet = self.clean_tweet(tweet)\n",
    "        tweet_en = translator.translate(tweet,dest=\"en\")\n",
    "        blob = TextBlob(tweet_en.text,analyzer=NaiveBayesAnalyzer())\n",
    "        return blob.sentiment\n",
    "\n",
    "    def get_sentiment_tweet_pol_sub(self, tweet):\n",
    "        translator = Translator()\n",
    "        tweet = self.clean_tweet(tweet)\n",
    "        tweet_en = translator.translate(tweet,dest=\"en\")\n",
    "        blob = TextBlob(tweet_en.text)\n",
    "        return blob.sentiment\n",
    "\n",
    "    def get_tweets(self):\n",
    "        tweets = []\n",
    "        print(\"Fetching tweets...\")\n",
    "        tweets_data = self.read_file(self.hashtag, self.sinceDate, self.untilDate)\n",
    "        \n",
    "        for i in range(len(tweets_data)):\n",
    "            parsed_tweet = {}\n",
    "            try :\n",
    "                parsed_tweet['date'] = tweets_data.iloc[i]['date']\n",
    "                parsed_tweet['conversationId'] = tweets_data.iloc[i]['conversationId']\n",
    "                parsed_tweet['tweet'] = tweets_data.iloc[i]['content']\n",
    "                aux_class_tweet = self.get_sentiment_tweet(\n",
    "                    str(tweets_data.iloc[i]['content']))\n",
    "                aux_class_tweet_pol_sub = self.get_sentiment_tweet_pol_sub(\n",
    "                    str(tweets_data.iloc[i]['content']))\n",
    "                parsed_tweet['sentiment_classification'] = aux_class_tweet.classification\n",
    "                parsed_tweet['p_pos'] = aux_class_tweet.p_pos\n",
    "                parsed_tweet['p_neg'] = aux_class_tweet.p_neg\n",
    "                parsed_tweet['polarity'] = aux_class_tweet_pol_sub.polarity\n",
    "                parsed_tweet['subjectivity'] = aux_class_tweet_pol_sub.subjectivity\n",
    "                tweets.append(parsed_tweet)\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "def main():\n",
    "    scraper = TwitterScrapeSentimentAnalysis()\n",
    "    tweets = scraper.get_tweets()\n",
    "    df = pd.DataFrame(tweets)\n",
    "    df.to_csv('tweets_output_textBlob.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets...\n",
      "Reading file...\n",
      "Reading file...\n"
     ]
    }
   ],
   "source": [
    "class TwitterClientSentimentAnalysisVader():\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.sinceDate = \"2022-01-01\"\n",
    "            self.untilDate = \"2022-03-13\"\n",
    "            self.hashtag = \"\"\n",
    "            self.vaderObj = SentimentIntensityAnalyzer()\n",
    "        except:\n",
    "            print(\"Error: scraper Failed\")\n",
    "\n",
    "    def read_file(self, hashtag, since, until):\n",
    "        print(\"Reading file...\")\n",
    "        os.system(\"snscrape --jsonl --max-results 500 --since {since} twitter-hashtag '{hashtag} until:{until}' > text-query-tweets.json\".format(\n",
    "            hashtag=hashtag, since=since, until=until))\n",
    "        print(\"Reading file...\")\n",
    "        tweets = pd.read_json(\"text-query-tweets.json\", lines=True)\n",
    "        return tweets\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
    "\n",
    "    def get_sentiment_tweet(self, tweet):\n",
    "        translator = Translator()\n",
    "        tweet = self.clean_tweet(tweet)\n",
    "        tweet_en = translator.translate(tweet,dest=\"en\")\n",
    "        sentiment = self.vaderObj.polarity_scores(tweet_en.text)\n",
    "        return sentiment\n",
    "\n",
    "    def get_tweets(self):\n",
    "        tweets = []\n",
    "        print(\"Fetching tweets...\")\n",
    "        tweets_data = self.read_file(\n",
    "            self.hashtag, self.sinceDate, self.untilDate)\n",
    "\n",
    "        for i in range(len(tweets_data)):\n",
    "            try:\n",
    "                parsed_tweet = {}\n",
    "                parsed_tweet['date'] = tweets_data.iloc[i]['date']\n",
    "                parsed_tweet['conversationId'] = int(tweets_data.iloc[i]['conversationId'])\n",
    "                parsed_tweet['tweet'] = tweets_data.iloc[i]['content']\n",
    "                aux_vader_sent = self.get_sentiment_tweet(\n",
    "                    str(tweets_data.iloc[i]['content']))\n",
    "                parsed_tweet['compound'] = aux_vader_sent['compound']\n",
    "                if aux_vader_sent['compound'] >= 0.05 :\n",
    "                    parsed_tweet['classification'] = \"Positive\"\n",
    "                elif aux_vader_sent['compound'] <= - 0.05 :\n",
    "                    parsed_tweet['classification'] = \"Negative\"\n",
    "                else :\n",
    "                    parsed_tweet['classification'] = \"Neutral\"\n",
    "                parsed_tweet['neg'] = aux_vader_sent['neg']\n",
    "                parsed_tweet['neu'] = aux_vader_sent['neu']\n",
    "                parsed_tweet['pos'] = aux_vader_sent['pos']\n",
    "                    \n",
    "                tweets.append(parsed_tweet)\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "def main():\n",
    "    scraper = TwitterClientSentimentAnalysisVader()\n",
    "    tweets = scraper.get_tweets()\n",
    "    df = pd.DataFrame(tweets)\n",
    "    df.to_csv('tweets_sentiment_vader_output.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRCLEX PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets...\n",
      "Reading file...\n",
      "Reading file...\n"
     ]
    }
   ],
   "source": [
    "class TwitterClientSentimentAnalysisNRCLex(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.sinceDate = \"2022-01-01\"\n",
    "            self.untilDate = \"2022-03-13\"\n",
    "            self.hashtag = \"\"\n",
    "        except:\n",
    "            print(\"Error: scraper Failed\")\n",
    "\n",
    "    def read_file(self, hashtag, since, until):\n",
    "        print(\"Reading file...\")\n",
    "        os.system(\"snscrape --jsonl --max-results 500 --since {since} twitter-hashtag '{hashtag} until:{until}' > text-query-tweets.json\".format(\n",
    "            hashtag=hashtag, since=since, until=until))\n",
    "        print(\"Reading file...\")\n",
    "        tweets = pd.read_json(\"text-query-tweets.json\", lines=True)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
    "\n",
    "    def get_nrclex_tweet(self, tweet):\n",
    "        translator = Translator()\n",
    "        tweet = self.clean_tweet(tweet)\n",
    "        tweet_en = translator.translate(tweet,dest=\"en\")\n",
    "        tweetObj = NRCLex(tweet_en.text).affect_frequencies\n",
    "        return tweetObj\n",
    "\n",
    "    def get_tweets(self):\n",
    "        tweets = []\n",
    "        print(\"Fetching tweets...\")\n",
    "        tweets_data = self.read_file(\n",
    "            self.hashtag, self.sinceDate, self.untilDate)\n",
    "\n",
    "        for i in range(len(tweets_data)):\n",
    "            try:\n",
    "                parsed_tweet = {}\n",
    "                parsed_tweet['date'] = tweets_data.iloc[i]['date']\n",
    "                parsed_tweet['conversationId'] = int(\n",
    "                    tweets_data.iloc[i]['conversationId'])\n",
    "                parsed_tweet['tweet'] = tweets_data.iloc[i]['content']\n",
    "                aux_NRCLex_sent = self.get_nrclex_tweet(str(tweets_data.iloc[i]['content']))\n",
    "                parsed_tweet['emotion'] = max(aux_NRCLex_sent, key=aux_NRCLex_sent.get)\n",
    "                parsed_tweet['score'] = aux_NRCLex_sent[max(aux_NRCLex_sent, key=aux_NRCLex_sent.get)]\n",
    "                tweets.append(parsed_tweet)\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "def main():\n",
    "    scraper = TwitterClientSentimentAnalysisNRCLex()\n",
    "    tweets = scraper.get_tweets()\n",
    "    df = pd.DataFrame(tweets)\n",
    "    df.to_csv('tweets_sentiment_NRCLex_output.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER SENTIMENT MODULE PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/shahules786/Twitter-Sentiment/releases/download/v1.0/classifer_model.pt\" to /home/blindedsoul/.cache/torch/checkpoints/classifer_model.pt\n",
      "100%|██████████| 178M/178M [00:14<00:00, 12.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets...\n",
      "Reading file...\n",
      "Reading file...\n"
     ]
    }
   ],
   "source": [
    "class TwitterClientSentimentAnalysisTwitterModule(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.sinceDate = \"2022-01-01\"\n",
    "            self.untilDate = \"2022-03-13\"\n",
    "            self.hashtag = \"\"\n",
    "            self.sent = TwitterSentiment.Sentiment()\n",
    "            self.sent.load_pretrained()\n",
    "        except:\n",
    "            print(\"Error: Authentication Failed\")\n",
    "\n",
    "    def read_file(self, hashtag, since, until):\n",
    "        print(\"Reading file...\")\n",
    "        os.system(\"snscrape --jsonl --max-results 500 --since {since} twitter-hashtag '{hashtag} until:{until}' > text-query-tweets.json\".format(\n",
    "            hashtag=hashtag, since=since, until=until))\n",
    "        print(\"Reading file...\")\n",
    "        tweets = pd.read_json(\"text-query-tweets.json\", lines=True)\n",
    "        return tweets\n",
    "        \n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
    "\n",
    "    def get_twitter_tweet(self, tweet):\n",
    "        translator = Translator()\n",
    "        tweet = self.clean_tweet(tweet)\n",
    "        tweet_en = translator.translate(tweet,dest=\"en\")\n",
    "        sentiment = self.sent.predict(tweet_en.text)\n",
    "        return sentiment\n",
    "\n",
    "    def get_tweets(self):\n",
    "        tweets = []\n",
    "        print(\"Fetching tweets...\")\n",
    "        tweets_data = self.read_file(\n",
    "            self.hashtag, self.sinceDate, self.untilDate)\n",
    "        for i in range(len(tweets_data)):\n",
    "            parsed_tweet = {}\n",
    "            parsed_tweet['date'] = tweets_data.iloc[i]['date']\n",
    "            parsed_tweet['conversationId'] = tweets_data.iloc[i]['conversationId']\n",
    "            parsed_tweet['tweet'] = tweets_data.iloc[i]['content']\n",
    "            aux_twitter_sent = self.get_twitter_tweet(\n",
    "                str(tweets_data.iloc[i]['content']))\n",
    "            if aux_twitter_sent > 0.5 :\n",
    "                parsed_tweet['classification'] = \"Negative\"\n",
    "            elif aux_twitter_sent < 0.5 :\n",
    "                parsed_tweet['classification'] = \"Positive\"\n",
    "            else :\n",
    "                parsed_tweet['classification'] = \"Neutral\"\n",
    "            parsed_tweet['score'] = aux_twitter_sent\n",
    "            tweets.append(parsed_tweet)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "def main():\n",
    "    scraper = TwitterClientSentimentAnalysisTwitterModule()\n",
    "    tweets = scraper.get_tweets()\n",
    "    df = pd.DataFrame(tweets)\n",
    "    df.to_csv('tweets_sentiment_twitter_module_output.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAIR PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17 18:14:00,396 loading file /home/blindedsoul/.flair/models/sentiment-en-mix-distillbert_4.pt\n",
      "Fetching tweets...\n",
      "Reading file...\n",
      "Reading file...\n"
     ]
    }
   ],
   "source": [
    "class TwitterClientSentimentAnalysisFlair(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.sinceDate = \"2022-01-01\"\n",
    "            self.untilDate = \"2022-03-13\"\n",
    "            self.hashtag = \"\"\n",
    "            self.classifier = TextClassifier.load('en-sentiment')\n",
    "        except:\n",
    "            print(\"Error: Authentication Failed\")\n",
    "\n",
    "\n",
    "    def read_file(self, hashtag, since, until):\n",
    "        print(\"Reading file...\")\n",
    "        os.system(\"snscrape --jsonl --max-results 500 --since {since} twitter-hashtag '{hashtag} until:{until}' > text-query-tweets.json\".format(\n",
    "            hashtag=hashtag, since=since, until=until))\n",
    "        print(\"Reading file...\")\n",
    "        tweets = pd.read_json(\"text-query-tweets.json\", lines=True)\n",
    "        return tweets\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
    "\n",
    "    def get_flair_tweet(self, tweet):\n",
    "        translator = Translator()\n",
    "        tweet = self.clean_tweet(tweet)\n",
    "        tweet_en = translator.translate(tweet,dest=\"en\")\n",
    "        sentence = Sentence(tweet_en.text)\n",
    "        self.classifier.predict(sentence)\n",
    "        return sentence.labels[0]\n",
    "\n",
    "    def get_tweets(self):\n",
    "        tweets = []\n",
    "        print(\"Fetching tweets...\")\n",
    "        tweets_data = self.read_file(\n",
    "            self.hashtag, self.sinceDate, self.untilDate)\n",
    "        for i in range(len(tweets_data)):\n",
    "            parsed_tweet = {}\n",
    "            parsed_tweet['date'] = tweets_data.iloc[i]['date']\n",
    "            parsed_tweet['conversationId'] = tweets_data.iloc[i]['conversationId']\n",
    "            parsed_tweet['tweet'] = tweets_data.iloc[i]['content']\n",
    "            aux_flair_sent = self.get_flair_tweet(str(tweets_data.iloc[i]['content']))\n",
    "            parsed_tweet['sentiment'] = str(aux_flair_sent).split(\" \")[0]\n",
    "            parsed_tweet['score'] = str(aux_flair_sent).split(\" \")[1][1:-1]\n",
    "            tweets.append(parsed_tweet)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "def main():\n",
    "    scraper = TwitterClientSentimentAnalysisFlair()\n",
    "    tweets = scraper.get_tweets()\n",
    "    df = pd.DataFrame(tweets)\n",
    "    df.to_csv('tweets_sentiment_flair_output.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT2EMOTION PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets...\n",
      "Reading file...\n",
      "Reading file...\n"
     ]
    }
   ],
   "source": [
    "class TwitterClientEmotionalAnalysis(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.sinceDate = \"2022-01-01\"\n",
    "            self.untilDate = \"2022-03-13\"\n",
    "            self.hashtag = \"\"\n",
    "        except:\n",
    "            print(\"Error: Authentication Failed\")\n",
    "\n",
    "    def read_file(self, hashtag, since, until):\n",
    "        print(\"Reading file...\")\n",
    "        os.system(\"snscrape --jsonl --max-results 500 --since {since} twitter-hashtag '{hashtag} until:{until}' > text-query-tweets.json\".format(\n",
    "            hashtag=hashtag, since=since, until=until))\n",
    "        print(\"Reading file...\")\n",
    "        tweets = pd.read_json(\"text-query-tweets.json\", lines=True)\n",
    "        return tweets\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
    "\n",
    "    def get_emotion_tweet(self, tweet):\n",
    "        translator = Translator()\n",
    "        tweet = self.clean_tweet(tweet)\n",
    "        tweet_en = translator.translate(tweet,dest=\"en\")\n",
    "        emotions = te.get_emotion(tweet_en.text)\n",
    "        return emotions\n",
    "\n",
    "    def get_tweets(self):\n",
    "        tweets = []\n",
    "        print(\"Fetching tweets...\")\n",
    "        tweets_data = self.read_file(\n",
    "            self.hashtag, self.sinceDate, self.untilDate)\n",
    "        for i in range(len(tweets_data)):\n",
    "            try:\n",
    "                parsed_tweet = {}\n",
    "                parsed_tweet['date'] = tweets_data.iloc[i]['date']\n",
    "                parsed_tweet['conversationId'] = tweets_data.iloc[i]['conversationId']\n",
    "                parsed_tweet['tweet'] = tweets_data.iloc[i]['content']\n",
    "                aux_emotions = self.get_emotion_tweet(str(tweets_data.iloc[i]['content']))\n",
    "                parsed_tweet['Angry'] =  aux_emotions['Angry']\n",
    "                parsed_tweet['Fear'] =  aux_emotions['Fear']\n",
    "                parsed_tweet['Happy'] =  aux_emotions['Happy']\n",
    "                parsed_tweet['Sad'] =  aux_emotions['Sad']\n",
    "                parsed_tweet['Surprise'] =  aux_emotions['Surprise']\n",
    "                tweets.append(parsed_tweet)\n",
    "            except KeyError  as e:\n",
    "                print(e)\n",
    "        return tweets\n",
    "\n",
    "\n",
    "def main():\n",
    "    scraper = TwitterClientEmotionalAnalysis()\n",
    "    tweets = scraper.get_tweets()\n",
    "    df = pd.DataFrame(tweets)\n",
    "    df.to_csv('tweets_emotions_text2emotions_output.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
